-- ============================================================
-- AWS Certified AI Practitioner (AIF-C01) 세트 1
-- 도메인: 책임 있는 AI에 대한 가이드라인 - 예상 기출문제 8개 (문제 36-43)
-- Supabase SQL Editor에서 실행하세요
-- ============================================================


-- ── 문제 36: 책임감 있는 AI 특성 + Amazon Bedrock Guardrails ──
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q136',
  'aws-aif-c01',
  '한 핀테크 기업이 Amazon Bedrock으로 대출 심사 AI 챗봇을 구축하려 합니다. 다음 요구사항을 충족해야 합니다.

[요구사항]
• 인종, 성별, 나이에 따른 차별적 대출 결정 금지
• 금리·수수료 등 금융 정보는 반드시 사실에 근거한 정확한 정보만 제공
• 악의적 입력(적대적 공격)에도 일관된 결과 유지
• 부적절한 콘텐츠(욕설, 음란물, 혐오 발언) 자동 차단
• 개인 금융 정보를 응답에 노출하지 않음

다음 중 이 요구사항을 충족하는 책임감 있는 AI의 특성과 AWS 도구를 가장 올바르게 설명한 것은?',
  'b',
  '책임감 있는 AI는 단일 특성이 아닌 여러 특성의 조합으로 구현됩니다.

요구사항 매핑:
1) 차별 금지 → 공정성(Fairness): 모든 인구통계 그룹에 균등한 처우
2) 사실 기반 정보 → 진실성(Truthfulness): 검증된 정보만 제공, 환각 방지
3) 적대적 입력 대응 → 견고성(Robustness): 비정상 입력에도 안정적 작동
4) 부적절 콘텐츠 차단 → 안전성(Safety): 유해 출력 방지
5) 개인정보 보호 → 포용성·프라이버시

Amazon Bedrock Guardrails 기능:
• 콘텐츠 필터(Content Filters): 유해 콘텐츠 분류 및 차단 (증오, 폭력, 성인)
• 민감 정보 레독션(Sensitive Information Redaction): 개인정보 자동 마스킹
• 주제 거부(Denied Topics): 특정 주제 응답 금지 설정
• 단어 필터(Word Filters): 특정 단어/문구 차단
• 그라운딩 체크(Grounding Check): 환각 방지 - 응답이 근거 문서에 기반하는지 확인',
  2,
  '책임감 있는 AI 핵심 특성:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
공정성 (Fairness):
• 인종, 성별, 나이, 종교 등에 따른 차별 금지
• 모든 그룹에 동등한 결과 제공
• 측정: 인구통계별 성능 차이 최소화
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
포용성 (Inclusivity):
• 다양한 배경과 능력의 사용자 고려
• 접근성, 다국어 지원, 다양한 입력 형태 지원
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
견고성 (Robustness):
• 예외적·적대적 입력에도 안정적 작동
• 노이즈, 이상값, 공격에 내성
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
안전성 (Safety):
• 유해한 출력 방지 (폭력, 혐오, 음란)
• 사용자와 사회에 해를 끼치지 않음
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
진실성 (Truthfulness):
• 사실에 기반한 정보만 제공
• 환각(Hallucination) 최소화
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Amazon Bedrock Guardrails 주요 기능:
• 콘텐츠 필터 / 민감 정보 레독션
• 주제 거부 / 단어 필터 / 그라운딩 체크',
  '[{"name": "Amazon Bedrock Guardrails", "url": "https://aws.amazon.com/bedrock/guardrails/"}, {"name": "책임감 있는 AI AWS", "url": "https://aws.amazon.com/machine-learning/responsible-ai/"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q136', 'a', '모델 정확도(Accuracy)만 최대화하면 공정성과 안전성은 자동으로 보장된다', '정확도가 높아도 특정 그룹에 편향될 수 있고, 유해 콘텐츠를 정확하게 생성할 수도 있습니다. 책임감 있는 AI는 정확도 외 추가 특성이 필요합니다.', 1),
  ('awsaifc01-q136', 'b', '공정성(차별 금지), 진실성(사실 기반 정보), 견고성(적대적 입력 대응), 안전성(유해 콘텐츠 차단)을 갖추고, Amazon Bedrock Guardrails의 콘텐츠 필터·민감 정보 레독션·주제 거부·그라운딩 체크로 각 요구사항을 기술적으로 구현한다', '책임감 있는 AI의 4가지 핵심 특성(공정성, 진실성, 견고성, 안전성)과 Bedrock Guardrails의 구체적 기능이 요구사항에 정확히 매핑됩니다.', 2),
  ('awsaifc01-q136', 'c', 'Amazon SageMaker Clarify만 사용하면 모든 책임감 있는 AI 요구사항을 충족할 수 있다', 'SageMaker Clarify는 편향 탐지와 설명 가능성 도구이며, 콘텐츠 필터링이나 실시간 안전 가드레일 기능은 없습니다.', 3),
  ('awsaifc01-q136', 'd', '책임감 있는 AI 특성 중 안전성(Safety)만 구현하면 나머지 특성은 선택적으로 적용해도 된다', '금융 AI에서 공정성, 진실성, 견고성은 법적 의무 사항이 될 수 있습니다. 안전성 하나만으로는 부족합니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q136', '책임 있는 AI에 대한 가이드라인');


-- ── 문제 37: GenAI 사용의 법적·비즈니스 위험 파악 ──────────
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q137',
  'aws-aif-c01',
  '한 글로벌 미디어 기업이 GenAI를 마케팅 콘텐츠 제작에 활용하고 있습니다. 법무팀이 다음 4가지 상황을 발견했습니다.

[발견된 상황]
• 상황 1: AI가 생성한 광고 이미지가 유명 사진작가의 저작물과 구도·스타일이 매우 유사함
• 상황 2: AI가 경쟁 제품의 성능을 허위로 비하하는 비교 광고 문구를 생성함 (확인되지 않은 사실 포함)
• 상황 3: 고객 프롬프트에 입력된 개인정보(이름, 주소)가 AI 생성 마케팅 이메일에 그대로 노출됨
• 상황 4: AI가 생성한 채용 광고가 특정 성별을 선호하는 표현을 반복적으로 사용함

다음 중 이 상황들의 법적·비즈니스 위험을 가장 올바르게 분석한 것은?',
  'c',
  '각 상황의 위험 분석:

상황 1 - 지적 재산권(IP) 침해 위험:
GenAI가 학습 데이터의 저작물과 유사한 콘텐츠를 생성할 수 있습니다. 저작권 침해 소송 위험이 있으며, 기업이 법적 책임을 질 수 있습니다.

상황 2 - 환각(Hallucination)으로 인한 허위 정보 위험:
AI가 사실로 확인되지 않은 정보를 생성하여 허위 광고, 명예훼손, 소비자 보호법 위반으로 이어질 수 있습니다.

상황 3 - 개인정보 보호 및 최종 사용자 위험:
고객 데이터가 AI 출력에 노출되면 GDPR, 개인정보보호법 위반 및 고객 신뢰 손실로 이어집니다.

상황 4 - 편향된 모델 출력으로 인한 차별 위험:
성별 편향적 채용 광고는 고용 차별 관련 법률 위반 및 브랜드 이미지 손상을 초래합니다.

이 4가지는 모두 GenAI 사용의 실제 법적·비즈니스 위험 사례입니다.',
  2,
  'GenAI 법적·비즈니스 위험 유형:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
지적 재산권(IP) 침해:
• 학습 데이터의 저작물과 유사한 생성물
• 저작권, 상표권, 특허 침해 가능
• 대응: 훈련 데이터 라이선스 확인
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
환각(Hallucination)으로 인한 허위 정보:
• 확인되지 않은 사실 생성
• 허위 광고, 명예훼손, 소비자 피해
• 대응: 인간 검토, 사실 검증 파이프라인
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
개인정보 유출 (최종 사용자 위험):
• 입력 데이터가 출력에 노출
• GDPR, 개인정보보호법 위반
• 대응: Bedrock Guardrails 민감 정보 레독션
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
편향된 출력 (차별 위험):
• 특정 그룹에 불리한 콘텐츠 생성
• 고용 차별법, 평등법 위반
• 대응: SageMaker Clarify로 편향 감지
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
고객 신뢰 손실:
• AI 오류로 인한 브랜드 이미지 손상
• 고객 이탈, 비즈니스 손실
• 대응: 인간 감독(Human in the Loop)',
  '[{"name": "AWS GenAI 책임감 있는 사용", "url": "https://aws.amazon.com/machine-learning/responsible-ai/"}, {"name": "Amazon Bedrock 데이터 보안", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/security.html"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q137', 'a', '상황 1의 IP 침해만이 법적 위험이며, 나머지 상황은 GenAI의 일반적인 특성으로 법적 문제가 아니다', '환각으로 인한 허위 정보, 개인정보 유출, 차별적 콘텐츠는 모두 실질적인 법적 위험입니다.', 1),
  ('awsaifc01-q137', 'b', 'GenAI가 생성한 콘텐츠에 대한 법적 책임은 AI 모델 제공업체(AWS 등)에게만 있으므로 기업은 걱정할 필요없다', '일반적으로 AI 서비스 계약에서 콘텐츠에 대한 법적 책임은 사용자(기업)에게 있습니다. AWS는 인프라를 제공하지 콘텐츠를 보증하지 않습니다.', 2),
  ('awsaifc01-q137', 'c', '4가지 상황 모두 실제 GenAI 법적·비즈니스 위험이다: 상황1-IP 침해 소송, 상황2-환각으로 인한 허위 광고 및 명예훼손, 상황3-개인정보 보호법 위반, 상황4-성별 차별로 인한 고용 차별법 위반 위험이다', '4가지 모두 GenAI 도입 기업이 실제로 직면하는 법적·비즈니스 위험입니다. 각각 다른 법률 영역과 규정 준수 요구사항이 관련됩니다.', 3),
  ('awsaifc01-q137', 'd', 'GenAI 사용 중 발생하는 모든 문제는 AI 기술의 일반적 한계이므로 법적 책임이 없다', 'AI 기술의 한계는 법적 면책 사유가 되지 않습니다. 기업은 AI 시스템의 출력에 대해 법적 책임을 집니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q137', '책임 있는 AI에 대한 가이드라인');


-- ── 문제 38: 데이터셋 특성과 편향 (포용성, 다양성, 균형) ─────
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q138',
  'aws-aif-c01',
  '한 HR 스타트업이 이력서 자동 선별 AI를 개발하고 있습니다. 훈련 데이터는 5개 대형 IT 기업의 10년간 채용 합격 이력서로 구성되어 있습니다. 데이터 분석 결과가 아래와 같습니다.

[데이터 분석 결과]
• 합격 이력서의 85%: 남성
• 합격 이력서의 95%: 22~35세 연령대
• 합격 이력서의 3%만: 소수 민족 그룹
• 출신 학교 패턴: 상위 20개 대학 출신이 78%
• 직전 직장 규모: 직원 500인 이상 대기업 출신이 90%

다음 중 이 데이터셋의 문제와 영향을 가장 올바르게 설명한 것은?',
  'c',
  '이 데이터셋은 심각한 구조적 편향을 내포하고 있습니다.

문제 분석:
1) 성별 편향: 85% 남성 → AI가 여성 지원자를 체계적으로 낮게 평가할 가능성
2) 연령 편향: 95%가 22~35세 → 36세 이상 숙련 인재를 부당하게 제외할 가능성
3) 민족적 편향: 3% 소수 민족 → 다양한 민족 배경 지원자에게 불이익
4) 학력 편향: 특정 대학 선호 → 출신 대학보다 능력을 보는 기회 박탈
5) 회사 규모 편향: 중소기업 경력자 불이익

근본 원인: 과거의 채용 결정이 이미 편향되어 있었으며, 이 편향을 AI가 학습하고 증폭시킵니다.

데이터셋 개선 방향:
• 포용성(Inclusivity): 다양한 배경의 후보자 포함
• 다양성(Diversity): 성별, 연령, 민족, 학력의 다양성 확보
• 균형(Balanced Dataset): 각 그룹의 비율 조정
• 큐레이션(Curation): 채용 결과와 실제 직무 성과를 연계한 데이터 구성',
  2,
  '데이터셋 특성의 중요성:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
포용성 (Inclusivity):
• 소외된 그룹도 훈련 데이터에 충분히 포함
• 모든 사용자 유형에 적절히 작동하는 모델 개발
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
다양성 (Diversity):
• 성별, 연령, 민족, 지역, 문화적 다양성
• 다양한 관점과 경험 반영
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
균형 잡힌 데이터셋 (Balanced Dataset):
• 클래스/그룹 간 적절한 비율 유지
• 불균형 → 다수 클래스 편향 발생
• 기법: 오버샘플링, 언더샘플링, SMOTE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
큐레이팅된 데이터 소스 (Curated Data Sources):
• 품질 검증된 신뢰할 수 있는 소스
• 과거 편향된 결정 기록 제거
• 도메인 전문가 검토 포함
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
데이터 편향의 악순환:
편향된 역사적 데이터 → 편향된 AI 모델
→ 편향된 의사결정 → 더 편향된 데이터 생성',
  '[{"name": "SageMaker Clarify 편향 감지", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-detect-data-bias.html"}, {"name": "AWS 책임감 있는 AI 데이터 가이드", "url": "https://aws.amazon.com/machine-learning/responsible-ai/"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q138', 'a', '데이터 양이 충분히 많으므로 품질 문제는 자동으로 해결된다. 10년간의 데이터는 신뢰할 수 있다', '데이터 양이 많아도 편향된 패턴이 더 강화될 뿐입니다. 10년간의 편향된 채용 결정은 AI가 그 편향을 학습하고 증폭시킵니다.', 1),
  ('awsaifc01-q138', 'b', '이 데이터셋은 실제 채용 결과를 반영하므로 객관적이다. AI는 과거 데이터에서 자연스러운 패턴을 학습한다', '과거의 채용 결정 자체가 이미 편향되어 있었습니다. 편향된 역사적 데이터를 학습하면 AI가 그 편향을 재현하고 자동화합니다.', 2),
  ('awsaifc01-q138', 'c', '이 데이터셋은 포용성(Inclusivity), 다양성(Diversity), 균형(Balanced Dataset)이 심각하게 부족하다. AI가 여성, 고령, 소수 민족, 비명문대 출신, 중소기업 경력자를 체계적으로 차별하게 될 위험이 크며, 이는 고용 차별 법률 위반으로 이어질 수 있다', '데이터셋의 심각한 인구통계학적 편향이 AI 모델의 차별적 의사결정으로 직결되는 문제를 정확히 진단하고 있습니다.', 3),
  ('awsaifc01-q138', 'd', '데이터 편향은 알고리즘으로 완전히 보정할 수 있으므로 데이터 수집 단계의 다양성은 중요하지 않다', '알고리즘적 편향 보정은 제한적입니다. 근본적인 데이터 품질과 대표성 문제는 데이터 수집 단계에서 해결해야 합니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q138', '책임 있는 AI에 대한 가이드라인');


-- ── 문제 39: 편향 감지·모니터링 도구 (SageMaker Clarify, Model Monitor, A2I) ──
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q139',
  'aws-aif-c01',
  '한 의료보험 회사가 환자 위험도 예측 모델을 Amazon SageMaker로 배포했습니다. 의료 감독팀이 다음 세 가지 우려 사항을 제기했습니다.

[우려 사항]
• 우려 A: 모델이 연령대별, 성별별로 예측 정확도가 다를 수 있다 (예: 노인 환자 그룹에서 성능이 낮을 가능성)
• 우려 B: 모델 배포 후 시간이 지남에 따라 환자 데이터 패턴이 변화하여 모델 성능이 저하될 수 있다 (데이터/모델 드리프트)
• 우려 C: 모델 신뢰도가 낮은 경우(경계선 예측)에는 AI의 판단만으로 결정하지 않고 의사의 검토가 필요하다

다음 중 각 우려 사항에 적합한 AWS 서비스를 올바르게 매칭한 것은?',
  'b',
  '각 우려에 대한 도구 매핑:

우려 A (인구통계 그룹별 편향) → Amazon SageMaker Clarify:
Clarify는 훈련 전 데이터 편향과 훈련 후 모델 편향을 모두 분석합니다. 연령, 성별 등 민감 특성에 따른 예측 불공정성을 수치화하고 (DPL, DI, CDDL 지표 제공), SHAP 값 기반 개별 예측 설명도 제공합니다.

우려 B (데이터/모델 드리프트) → Amazon SageMaker Model Monitor:
Model Monitor는 프로덕션 환경에서 모델 성능과 데이터 품질을 지속 모니터링합니다. 데이터 드리프트(입력 데이터 분포 변화), 모델 드리프트(성능 저하), 편향 드리프트를 실시간 감지하고 알림을 보냅니다.

우려 C (저신뢰도 예측 인간 검토) → Amazon Augmented AI (A2I):
A2I는 AI 예측에 인간 검토 루프를 추가합니다. 신뢰도 임계값 이하의 예측만 의사에게 라우팅하여 인간-AI 협력 워크플로를 구성합니다.',
  3,
  '책임감 있는 AI 모니터링 도구:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Amazon SageMaker Clarify:
• 훈련 전: 데이터 편향 분석 (그룹별 불균형)
• 훈련 후: 모델 편향 분석 (그룹별 예측 차이)
• 편향 지표: DPL(차이 긍정 레이블 비율), DI(불평등 영향)
• 설명 가능성: SHAP 값 기반 특성 기여도 분석
• 하위 그룹 분석: 인구통계별 성능 세분화
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Amazon SageMaker Model Monitor:
• 데이터 품질 모니터링: 입력 데이터 통계 변화 감지
• 모델 품질 모니터링: 정확도, F1 등 성능 추적
• 편향 드리프트: 실시간 편향 변화 감지
• 기능 중요도 드리프트: 특성 영향 변화 감지
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Amazon Augmented AI (A2I):
• Human in the Loop 워크플로 구현
• 신뢰도 임계값 기반 인간 검토 자동 라우팅
• 의료, 금융, 법률 고위험 의사결정에 적합
• Amazon Mechanical Turk 또는 자체 검토팀 연결
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
레이블 품질 분석:
• Amazon SageMaker Ground Truth Plus
• 레이블러 간 일치도(Inter-annotator Agreement) 측정',
  '[{"name": "Amazon SageMaker Clarify", "url": "https://aws.amazon.com/sagemaker/clarify/"}, {"name": "SageMaker Model Monitor", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"}, {"name": "Amazon Augmented AI (A2I)", "url": "https://aws.amazon.com/augmented-ai/"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q139', 'a', '우려A-SageMaker Model Monitor, 우려B-Amazon A2I, 우려C-SageMaker Clarify', '모델 드리프트 감지는 Model Monitor, 편향 분석은 Clarify, 인간 검토는 A2I가 담당합니다. 매핑이 잘못되었습니다.', 1),
  ('awsaifc01-q139', 'b', '우려A-SageMaker Clarify(인구통계별 편향 분석), 우려B-SageMaker Model Monitor(데이터·모델 드리프트 실시간 감지), 우려C-Amazon Augmented AI/A2I(저신뢰도 예측 인간 검토)', '각 도구가 우려 사항에 정확히 매핑됩니다: Clarify→편향 분석, Model Monitor→드리프트 감지, A2I→인간 검토 루프.', 2),
  ('awsaifc01-q139', 'c', '모든 우려 사항은 Amazon Bedrock Guardrails 하나로 해결할 수 있다', 'Bedrock Guardrails는 생성형 AI 콘텐츠 필터링 도구입니다. 전통적 ML 모델의 편향 분석, 드리프트 감지, 인간 검토 워크플로는 별도 도구가 필요합니다.', 3),
  ('awsaifc01-q139', 'd', '모델 배포 후 모니터링은 불필요하다. 초기 훈련 시 편향을 제거하면 배포 후에는 문제가 없다', '배포 후 실세계 데이터는 지속적으로 변화합니다. 드리프트, 편향 변화, 새로운 패턴에 대응하기 위해 지속적 모니터링이 필수입니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q139', '책임 있는 AI에 대한 가이드라인');


-- ── 문제 40: 과적합·과소적합과 인구통계학적 영향 ─────────────
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q140',
  'aws-aif-c01',
  '한 은행이 소비자 대출 심사 ML 모델을 배포한 후 다음과 같은 성능 데이터를 확인했습니다.

[모델 성능 데이터]
• 훈련 데이터 정확도: 97%
• 검증 데이터 정확도: 73%
• 도시 거주자 예측 정확도: 91%
• 농촌 거주자 예측 정확도: 64%
• 25~40세 그룹 예측 정확도: 89%
• 55세 이상 그룹 예측 정확도: 61%
• 남성 지원자 대출 승인율: 72%
• 동일 신용 점수 여성 지원자 승인율: 58%

다음 중 이 모델의 문제점과 영향을 가장 올바르게 분석한 것은?',
  'c',
  '이 모델은 두 가지 주요 문제를 동시에 보이고 있습니다.

1) 과적합(Overfitting):
훈련 정확도 97% vs 검증 정확도 73%의 24% 차이는 모델이 훈련 데이터를 암기하여 새로운 데이터에 일반화하지 못함을 의미합니다. 모델이 훈련 데이터의 노이즈와 특이성까지 학습했습니다.

2) 인구통계학적 편향:
• 지역 편향: 농촌 거주자 64% vs 도시 91% (27% 차이) → 농촌 지역 주민이 부정확한 대출 심사 결과를 받음
• 연령 편향: 55세 이상 61% vs 25~40세 89% (28% 차이) → 고령자가 신용 서비스 접근에 불이익
• 성별 편향: 동일 신용 점수에서 여성 58% vs 남성 72% → 명백한 성별 차별

이 편향들은 금융 서비스 관련 차별 금지법(미국 ECOA, 국내 신용정보법 등) 위반 위험이 있습니다.',
  3,
  '편향과 분산 (Bias-Variance Trade-off):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
과적합 (Overfitting = 고분산, Low Bias):
• 증상: 훈련 정확도↑, 검증/테스트 정확도↓ (큰 차이)
• 원인: 모델이 너무 복잡, 훈련 데이터 암기
• 해결: 정규화, 드롭아웃, 더 많은 훈련 데이터, 조기 종료
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
과소적합 (Underfitting = 고편향, High Bias):
• 증상: 훈련, 검증 정확도 모두 낮음
• 원인: 모델이 너무 단순, 데이터의 패턴 포착 못함
• 해결: 더 복잡한 모델, 특성 추가, 훈련 시간 증가
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
인구통계학적 편향의 영향:
• 특정 그룹에 불이익: 서비스 접근 제한
• 법적 위험: 차별 금지법 위반 가능
• 사회적 불평등 심화: AI가 기존 불평등 강화
• 비즈니스 위험: 규제 제재, 소송, 신뢰 손실
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
하위 그룹 분석 (Subgroup Analysis):
• 인구통계 그룹별 성능 분리 측정
• SageMaker Clarify: 자동화된 하위 그룹 편향 분석
• 균등한 성능을 위한 공정성 제약 조건 추가',
  '[{"name": "SageMaker Clarify 편향 메트릭", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html"}, {"name": "ML 공정성 가이드", "url": "https://aws.amazon.com/machine-learning/responsible-ai/"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q140', 'a', '훈련 정확도가 97%로 매우 높으므로 모델 성능은 우수하다. 그룹별 성능 차이는 데이터의 자연스러운 특성이다', '훈련 정확도만 보는 것은 위험합니다. 검증 정확도가 73%로 크게 낮아 과적합이 명확하며, 그룹별 성능 차이는 차별이지 자연스러운 특성이 아닙니다.', 1),
  ('awsaifc01-q140', 'b', '이 모델은 과소적합(Underfitting)을 보이고 있다. 더 복잡한 모델 아키텍처를 사용하면 해결된다', '과소적합은 훈련과 검증 정확도 모두 낮을 때입니다. 이 모델은 훈련 97%와 검증 73%의 큰 차이로 과적합을 보이고 있습니다.', 2),
  ('awsaifc01-q140', 'c', '이 모델은 과적합(훈련 97% vs 검증 73%)과 심각한 인구통계학적 편향(지역·연령·성별에 따른 성능 격차)을 동시에 보이고 있다. 농촌 주민, 고령자, 여성이 부정확한 대출 심사를 받아 금융 서비스에서 체계적으로 불이익을 받고 있다', '과적합과 다중 인구통계학적 편향의 복합 문제를 정확하게 진단하고, 그 비즈니스·사회적 영향까지 올바르게 설명하고 있습니다.', 3),
  ('awsaifc01-q140', 'd', '그룹별 성능 차이는 해당 그룹의 실제 신용 위험 차이를 반영하므로 편향이 아니다', '동일한 신용 점수에서 성별에 따라 승인율이 다른 것은 신용 위험과 무관한 차별입니다. 금융 규제 하에서 이는 불법입니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q140', '책임 있는 AI에 대한 가이드라인');


-- ── 문제 41: 환경적 고려사항과 지속 가능한 FM 선택 ───────────
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q141',
  'aws-aif-c01',
  '한 글로벌 리테일 기업이 상품 설명을 자동 분류(의류/전자/식품)하는 AI 시스템을 구축하려 합니다. IT 팀이 세 가지 모델 옵션을 검토하고 있습니다.

[모델 비교]
┌─────────────────┬────────┬──────────────────┬──────────────┐
│ 모델             │ 정확도  │ 탄소 발자국       │ 추론 비용    │
├─────────────────┼────────┼──────────────────┼──────────────┤
│ A. 대형 LLM      │ 98.5%  │ 매우 높음 (100x) │ $500/1M토큰 │
│ B. 미세조정 BERT │ 97.2%  │ 낮음 (1x)        │ $5/1M토큰   │
│ C. 최신 중형 FM  │ 97.8%  │ 중간 (10x)       │ $50/1M토큰  │
└─────────────────┴────────┴──────────────────┴──────────────┘

기업의 ESG 목표: 2030년까지 탄소 중립. 하루 처리량: 100만 건의 상품 설명. 요구 정확도: 95% 이상

책임감 있는 AI 관행과 환경적 고려사항을 기반으로 가장 적합한 선택은?',
  'b',
  '환경적 고려사항을 반영한 책임감 있는 모델 선택:

요구사항 검토:
- 최소 정확도: 95% → 세 모델 모두 충족 (97.2%, 97.8%, 98.5%)
- 정확도 차이: 최저(B) vs 최고(A) = 1.3% 차이 (미미)

환경적 분석:
- 모델 A: 탄소 100x, 비용 $500 → 하루 100만 건 = $500,000/일
- 모델 B: 탄소 1x, 비용 $5 → 하루 100만 건 = $5,000/일
- 모델 C: 탄소 10x, 비용 $50 → 하루 100만 건 = $50,000/일

결론: 미세조정 BERT(모델 B)가 최적
1) 정확도 97.2%로 요구사항(95% 이상) 충족
2) 탄소 발자국 1/100 (ESG 목표 부합)
3) 비용 1/100 (하루 $5,000 vs $500,000)
4) 단순 분류 작업에 대형 LLM은 과도한 선택

책임감 있는 모델 선택 원칙:
"충분히 좋은(good enough) 성능"의 소형 모델이 환경, 비용, 유지보수 측면에서 더 책임감 있는 선택입니다.',
  2,
  '책임감 있는 FM 선택: 환경적 고려사항:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
탄소 발자국 고려:
• 대형 모델 훈련: 수백 톤 CO2 배출
• 추론 비용: 사용량 비례 에너지 소비
• Right-sizing: 작업에 적합한 최소 크기 모델 선택
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
지속 가능성 원칙:
• Efficiency First: 동일 성능이면 작은 모델 선호
• Task-Appropriate: 작업 복잡도에 맞는 모델 선택
• 미세조정 > 대형 일반 모델 (특화 작업에서)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
AWS 지속 가능성 지원:
• AWS 재생 에너지 목표 (100% 재생 에너지)
• AWS 탄소 발자국 도구: 서비스별 탄소 배출 추적
• Graviton 프로세서: 동일 성능에 40% 에너지 절감
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
모델 선택 책임감 있는 관행:
• 과도한 모델 피하기 (Avoid Over-engineering)
• 성능 임계값 설정 (95% 이상이면 충분)
• 환경 비용 = 비즈니스 비용으로 인식
• ESG 목표와 AI 전략 정렬',
  '[{"name": "AWS 지속 가능성", "url": "https://sustainability.aboutamazon.com/"}, {"name": "AWS 탄소 발자국 도구", "url": "https://aws.amazon.com/aws-cost-management/aws-customer-carbon-footprint-tool/"}, {"name": "책임감 있는 AI AWS", "url": "https://aws.amazon.com/machine-learning/responsible-ai/"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q141', 'a', '가장 높은 정확도(98.5%)의 대형 LLM을 선택한다. AI 성능 극대화가 항상 최우선이다', '1.3% 추가 정확도를 위해 탄소 100배, 비용 100배를 사용하는 것은 무책임합니다. 요구사항(95% 이상)을 충족하는 한 환경 영향을 최소화해야 합니다.', 1),
  ('awsaifc01-q141', 'b', '정확도 요구사항(95% 이상)을 충족하는 미세조정 BERT(97.2%)를 선택한다. 대형 LLM 대비 탄소 발자국 1/100, 비용 1/100으로 ESG 목표와 책임감 있는 AI 원칙에 부합한다', '성능 요구사항을 충족하면서 환경 영향과 비용을 최소화하는 것이 책임감 있는 모델 선택의 핵심입니다.', 2),
  ('awsaifc01-q141', 'c', '환경적 고려사항은 AI 프로젝트에서 고려할 필요가 없다. 정확도와 비용만이 중요한 기준이다', '책임감 있는 AI 관행에는 환경적 지속 가능성이 포함됩니다. ESG 목표가 있는 기업은 AI 선택에서도 환경 영향을 반드시 고려해야 합니다.', 3),
  ('awsaifc01-q141', 'd', '오픈소스 모델을 사용하면 자동으로 환경 친화적이고 비용 효율적이다', '오픈소스 여부가 환경 친화성을 보장하지 않습니다. 오픈소스 대형 모델도 동일한 탄소 발자국을 가집니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q141', '책임 있는 AI에 대한 가이드라인');


-- ── 문제 42: 투명성·설명 가능성 vs 불투명한 모델 + SageMaker Model Cards ──
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q142',
  'aws-aif-c01',
  '한 기업이 직원 성과 평가에 AI 모델을 도입하려 합니다. 노동조합과 HR 책임자가 다음을 요구하고 있습니다.

[투명성 요구사항]
① 이 모델은 어떤 데이터로 훈련되었는가?
② AI는 어떤 기준으로 직원을 평가하는가?
③ 어떤 인구통계 그룹에서 편향이 있을 수 있는가?
④ 이 모델의 한계와 사용해서 안 되는 상황은?
⑤ 모델 라이선스와 지적재산권은 어떻게 되는가?

다음 중 이 요구사항을 충족하기 위한 가장 적절한 도구와 접근 방식은?',
  'c',
  'SageMaker Model Cards와 설명 가능한 AI 접근 방식이 이 요구사항을 종합적으로 충족합니다.

SageMaker Model Cards의 역할:
① 훈련 데이터: 데이터 출처, 수집 기간, 크기, 전처리 방법 문서화
② 모델 평가 기준: 모델 목적, 의도된 사용 사례, 알고리즘 설명
③ 편향 평가: SageMaker Clarify와 연동한 그룹별 성능 지표 포함
④ 한계와 경고: 적용하면 안 되는 상황, 알려진 편향, 정확도 한계 명시
⑤ 라이선스 정보: 모델과 데이터의 라이선스, IP 정보 기록

투명한 모델 vs 불투명한 모델:
• 투명: 의사결정 나무, 선형회귀 → 규칙 직접 확인 가능
• 설명 가능: 딥러닝 + SHAP/LIME → 후처리 설명 생성
• 불투명(블랙박스): 딥러닝만 사용 → 설명 없이 출력만 제공

오픈소스 모델의 투명성 장점: 아키텍처와 훈련 코드가 공개되어 감사(Audit) 가능',
  2,
  '설명 가능한 AI와 투명성 도구:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Amazon SageMaker Model Cards:
• 모델 목적, 훈련 데이터, 성능 지표 문서화
• 편향 평가 결과 포함
• 의도된/금지된 사용 사례 명시
• 라이선스 및 IP 정보 기록
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
투명한 모델 (Transparent Models):
• 의사결정 나무 (Decision Tree)
• 선형/로지스틱 회귀
• 규칙 기반 시스템
• 장점: 직접 해석 가능
• 단점: 복잡한 패턴 포착 제한
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
설명 가능한 AI 기법:
• SHAP (Shapley Additive exPlanations): 특성별 기여도
• LIME (Local Interpretable Model-agnostic Explanations): 지역적 설명
• Amazon SageMaker Clarify: SHAP 기반 설명 자동화
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
해석 가능성과 성능의 트레이드오프:
• 단순 모델: 해석 쉬움, 성능 낮을 수 있음
• 복잡 모델: 성능 높음, 해석 어려움
• 고위험 의사결정 (채용, 대출, 의료): 해석 가능성 우선
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
오픈소스 모델 투명성:
• 아키텍처 코드 공개 → 감사 가능
• 훈련 데이터·과정 공개 여부 확인 필요
• 라이선스 조건 확인 필수 (Apache, MIT, GPL 등)',
  '[{"name": "Amazon SageMaker Model Cards", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-cards.html"}, {"name": "SageMaker Clarify 설명 가능성", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-explainability.html"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q142', 'a', '모델의 정확도 수치만 공개하면 충분하다. 내부 작동 방식은 기업 영업 기밀로 보호해야 한다', '직원 성과 평가처럼 중요한 의사결정에서 정확도 하나만으로는 충분하지 않습니다. 편향, 한계, 사용 기준 공개가 법적·윤리적 의무일 수 있습니다.', 1),
  ('awsaifc01-q142', 'b', '블랙박스 딥러닝 모델은 어떤 방법으로도 설명이 불가능하므로 투명성은 원천적으로 불가능하다', 'SHAP, LIME, Amazon SageMaker Clarify 등 사후 설명 기법으로 블랙박스 모델도 어느 정도 설명 가능합니다.', 2),
  ('awsaifc01-q142', 'c', 'Amazon SageMaker Model Cards로 훈련 데이터 출처·모델 목적·성능 지표·편향 평가·한계·라이선스를 체계적으로 문서화하고, SageMaker Clarify의 SHAP 기반 설명으로 각 평가 결정의 근거를 제공한다', 'Model Cards로 모델의 전 생명주기를 문서화하고, Clarify로 개별 예측 설명을 제공하면 5가지 요구사항 모두 충족할 수 있습니다.', 3),
  ('awsaifc01-q142', 'd', '오픈소스 모델을 사용하기만 하면 자동으로 모든 투명성 요구사항이 충족된다', '오픈소스는 아키텍처 코드를 공개하지만, 훈련 데이터, 편향 평가, 사용 한계 등의 문서화는 별도로 수행해야 합니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q142', '책임 있는 AI에 대한 가이드라인');


-- ── 문제 43: 해석 가능성과 성능 트레이드오프 + 인간 중심 설계 ──
INSERT INTO questions (id, exam_id, text, correct_option_id, explanation, difficulty, key_points, ref_links)
VALUES (
  'awsaifc01-q143',
  'aws-aif-c01',
  '한 병원이 흉부 X-ray 영상으로 폐암 가능성을 예측하는 AI 시스템 도입을 검토하고 있습니다. 두 가지 모델이 최종 후보입니다.

[모델 A - 딥러닝 앙상블]
• 정확도: 94.8%
• 민감도(Sensitivity): 96%
• 예측 근거: 제공 불가 (블랙박스)
• 운영 방식: AI 단독 판정

[모델 B - 설명 가능한 CNN + 히트맵]
• 정확도: 89.2%
• 민감도(Sensitivity): 91%
• 예측 근거: X-ray에서 AI가 주목한 영역을 히트맵으로 시각화
• 운영 방식: AI + 방사선과 의사 최종 판정

책임감 있는 AI 원칙과 의료 안전 관점에서 어느 모델이 더 적합하며, 그 이유는 무엇입니까?',
  'b',
  '의료 고위험 환경에서는 해석 가능성이 성능 수치보다 더 중요할 수 있습니다.

모델 B가 더 적합한 이유:

1) 의사의 신뢰와 검증 가능성:
히트맵으로 AI가 어느 부위를 근거로 판단했는지 확인 가능 → 방사선과 의사가 AI 판단을 검토하고 동의/반대 결정 가능

2) 오류 발견 가능성:
AI가 실제 병변이 아닌 아티팩트(장비 노이즈, 마스크 그림자)를 보고 판단하는 경우 의사가 히트맵으로 발견 가능

3) 책임 소재 명확화:
의사가 AI 출력을 검토하고 최종 판정 → 법적 책임이 의사에게 명확
(블랙박스 AI 단독 판정은 책임 소재 불분명)

4) 환자 설명 권리:
"왜 암 의심이라고 판정했는가?"에 대한 설명 의무 충족

5% 정확도 차이의 재해석:
• 민감도 96% vs 91% → 1,000명 중 50명의 추가 발견
• 하지만 블랙박스 AI 단독 판정은 의사의 임상적 판단을 배제하여 오히려 위험할 수 있음
• 설명 가능한 AI + 의사 협력 = 실질적 최고 성능',
  3,
  '설명 가능한 AI와 인간 중심 설계 원칙:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
고위험 의사결정에서 해석 가능성:
• 의료, 법률, 금융, 채용: 설명 의무 있음
• EU AI Act: 고위험 AI는 설명 가능성 필수
• 미국 FDA: 의료 AI 투명성 규제 강화 추세
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
해석 가능성 vs 성능 트레이드오프:
• 단순 모델: 높은 해석 가능성, 낮을 수 있는 성능
• 복잡 모델: 낮은 해석 가능성, 높은 성능
• 해결책: 사후 설명 기법 (SHAP, LIME, Grad-CAM 히트맵)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
인간 중심 설계 (Human-Centered Design) 원칙:
• AI는 보조 도구, 인간 전문가가 최종 결정
• 사용자(의사)가 AI 출력을 이해하고 검증 가능
• 인간이 개입할 수 있는 Override 메커니즘 제공
• Human in the Loop: A2I, 의사 검토 워크플로
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
의료 AI 설명 기법:
• Grad-CAM: CNN의 히트맵 시각화
• SHAP: 특성별 기여도 분석
• LIME: 개별 예측 지역적 설명
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Amazon A2I: 의료 AI에서 인간 검토 루프 구현
SageMaker Clarify: 의료 모델 편향 분석',
  '[{"name": "AWS 의료 AI 책임감 있는 사용", "url": "https://aws.amazon.com/health/machine-learning/"}, {"name": "Amazon Augmented AI (A2I)", "url": "https://aws.amazon.com/augmented-ai/"}, {"name": "SageMaker Clarify 설명 가능성", "url": "https://aws.amazon.com/sagemaker/clarify/"}]'
);

INSERT INTO question_options (question_id, option_id, text, explanation, sort_order) VALUES
  ('awsaifc01-q143', 'a', '모델 A가 정확도(94.8%)와 민감도(96%)가 더 높으므로 의료 환경에서 항상 더 적합하다', '의료 고위험 환경에서 5% 높은 정확도가 블랙박스 AI 단독 판정의 위험성, 설명 불가능성, 의사 검토 배제보다 항상 우선하지는 않습니다.', 1),
  ('awsaifc01-q143', 'b', '모델 B가 더 적합하다. 히트맵으로 AI의 판단 근거를 방사선과 의사가 검토할 수 있어 오류 발견, 책임 명확화, 환자 설명 의무를 충족한다. 5% 성능 차이는 의사의 임상 판단이 AI를 보완하는 인간 중심 운영 방식으로 보완된다', '설명 가능성 + 인간 중심 운영(의사 최종 판정)의 조합이 책임감 있는 의료 AI의 핵심입니다. 성능과 안전성의 균형을 올바르게 설명합니다.', 2),
  ('awsaifc01-q143', 'c', '블랙박스 AI는 의료에서 절대 사용해서는 안 되므로 어떤 경우에도 모델 A는 도입 불가능하다', '블랙박스 AI도 적절한 인간 감독과 함께라면 의료에 활용될 수 있습니다. 절대적 금지보다는 적절한 안전장치가 중요합니다.', 3),
  ('awsaifc01-q143', 'd', '설명 가능한 모델은 성능이 낮으므로 의료처럼 중요한 분야에서는 사용할 수 없다', '설명 가능한 모델도 충분히 높은 성능(89~91%)을 가질 수 있으며, 의사의 임상 판단과 결합하면 더 높은 실질적 성능을 달성할 수 있습니다.', 4);

INSERT INTO question_tags (question_id, tag) VALUES ('awsaifc01-q143', '책임 있는 AI에 대한 가이드라인');


-- ── 세트 1에 책임 있는 AI 문제 8개 추가 (sort_order 36-43) ──
INSERT INTO exam_set_questions (set_id, question_id, sort_order) VALUES
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q136', 36),
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q137', 37),
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q138', 38),
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q139', 39),
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q140', 40),
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q141', 41),
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q142', 42),
  ('550e8400-e29b-41d4-a716-446655440001', 'awsaifc01-q143', 43);
